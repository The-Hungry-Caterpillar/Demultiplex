Trying to figure out which files contains which reads and which files contain which indexes...

1294_S1_L008_R1_001.fastq.gz --> contains read 1
1294_S1_L008_R2_001.fastq.gz --> contains index 1
1294_S1_L008_R3_001.fastq.gz --> contains index 2
1294_S1_L008_R4_001.fastq.gz --> contains read 2

#####################################################################################################################

Made some test files called:
R1.fastq
R2.fastq
R3.fastq
R4.fastq

They are just the first three records of the actual files that we will be reading, but with indexes made to satisfy the following:
1st indexes -- perfect match
2nd indexes -- swapped
3rd indexes -- bad quality

Note: THE INDEXES ARE REVERSE COMPLIMENTS

######################################################################################################################

Moving on to plotting the distributions

Successfully plotted the test files, moving on to actual files

Need to make a SLURM script for all four jobs

SLURM script complete. 

submitted SLURM:
	~$ sbatch distribution_SLURM.sh

	-$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15566945      bgmp distribu   jogata  R       1:18      1 n226

It didn't like the size of my array, trying a different method, resubmitted script
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15582549      bgmp distribu   jogata  R       1:27      1 n226

Okay that didn't work, I think it's because I was making a list that was the legnth of number of reads.
Redid script to have a running sum. 

Resubmitted job
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15631994      bgmp distribu   jogata  R       3:29      1 n227

This is taking forever, I resubmitted job with 8 cpus/task and 4 NODES
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15632365      bgmp distribu   jogata  R       0:10      4 n[226-229]

Okay don't ask for 4 nodes. Figured out my problem for real this time... I was trying to read through the file 101 times,
	so my code was taking 101 times longer than it should have. Fixed this problem.

Read 1 job:
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15633621      bgmp distribu   jogata  R       0:11      1 n225

Index 1 job:
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15633624      bgmp distribu   jogata  R       1:03      1 n226

Index 2 job:
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15633628      bgmp distribu   jogata  R       0:29      1 n227

Oops, forgot to change to f=gzip.open(), resubmitting....

submitted R1,I1,I2,R2, in that order: (distribution_SLURM, distribution_SLURM1, etc.)
 -$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15636052      bgmp distribu   jogata  R       0:04      1 n226
          15636051      bgmp distribu   jogata  R       0:08      1 n226
          15636050      bgmp distribu   jogata  R       0:11      1 n225
          15636049      bgmp distribu   jogata  R       0:17      1 n225

Okay jobs are complete.

Submitted sbatch for demux:
~$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15672906      bgmp demux.sh   jogata  R       1:56      1 n225

Uhhh, done? that was...fast, also why didn't my progress reports print?
	Command being timed: "python demux_script.py -i /projects/bgmp/shared/2017_sequencing/indexes.txt -bd buckets/ -r1 /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R1_001.fastq.gz -i1 /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R2_001.fastq.gz -i2 /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R3_001.fastq.gz -r2 /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R4_001.fastq.gz"
	User time (seconds): 10234.45
	System time (seconds): 70.46
	Percent of CPU this job got: 98%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 2:53:44
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 187688
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 1975609
	Voluntary context switches: 100572
	Involuntary context switches: 86751
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0

want to redo and put some bar plots at the end

After plotting it's obvious that too many reads went into bad qual -- going to rerun script with avg q score cutoff at 32
~$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15673959      bgmp demux.sh   jogata  R      52:48      1 n225


Current workflow is 'demux.sh' --> 'stats.sh' --> 'stats1.py', if you want to do a new run, move all buckets to new directory and change name of file 'data', preferably with Qscore cutoff

Okay workflow is condensed into demux.sh script.
UGH i overwrote all my buckets from Qscore cutoff of 32, rerun? still have the stats though.

running demux for qscore cutoff of 30:
 ~$ squeue -u jogata
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15676647      bgmp demux.sh   jogata  R       0:10      1 n225
		
Did a demux run with cutoff score of Q30, Q32, Q33, and Q35.
Put all the data and distributions into a folder called 'reports'